{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMS Q3 Project: Language detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project makes use of the Wikicorpus, a scraping of the English, Spanish, and Catalan versions of Wikipedia. It has over 750 million words.\n",
    "\n",
    "I downloaded the full English and Spanish texts. The Spanish data has about 120 million words, the English about 600 million. The end goal is to be able to type in some words in either Spanish or English and have a neural net recognize which one it is, and to then investigate the workings of the neural net to find out which features of the text are important to language detection.\n",
    "\n",
    "The project only makes use of the first 10,000 words in each set for training; it simply takes too long to process otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "file_content = [[],[]]\n",
    "\n",
    "#import files as readable objects\n",
    "for lang in ['en', 'es']:\n",
    "    #set up directories for finding files\n",
    "    dir = \"/Users/tulasiholdridge/Downloads/CMS Q3/{}\".format('raw.'+lang)    #path to files\n",
    "    \n",
    "    if lang=='en':\n",
    "        index=0\n",
    "    else:\n",
    "        index=1\n",
    "        \n",
    "    for file in sorted(os.listdir(dir)):\n",
    "        content = open(os.path.join(dir, file), encoding=\"latin-1\")\n",
    "        file_content[index].append(content)\n",
    "    \n",
    "    file_content = np.array(file_content)\n",
    "    \n",
    "raw_data = []\n",
    "labels = []\n",
    "\n",
    "#read first 10,000 words of each doc and add to raw dataset\n",
    "raw_data.append(file_content[0][0].read())\n",
    "labels.append('en')\n",
    "\n",
    "raw_data.append(' '.join([file_content[1][0].read(), file_content[1][1].read()]))\n",
    "labels.append('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<doc id=\"214730\" title=\"Henry Hallam\" nonfiltered=\"1\" processed=\"1\" dbindex=\"0\">\n",
      "Henry Hallam (July 9, 1777 - January 21, 1859) was an  English histor\n",
      "\n",
      "************\n",
      "\n",
      "<doc id=\"20540\" title=\"658\" nonfiltered=\"1\" processed=\"1\" dbindex=\"10000\">\n",
      "\n",
      " Acontecimientos .\n",
      "\n",
      "\n",
      " Nacimientos .\n",
      "\n",
      "\n",
      " Fallecimientos .\n",
      "Fulgencio de Écija\n"
     ]
    }
   ],
   "source": [
    "print(raw_data[0][:150])    #raw english text\n",
    "print('\\n************\\n')\n",
    "print(raw_data[1][:150])    #raw spanish text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup    #will help with text cleaning\n",
    "import re    #regex\n",
    "\n",
    "def process_wikicorpus(raw):\n",
    "    '''\n",
    "    Takes list where each item is a text. Returns processed version of said text (no html, punctuation, etc.) in \n",
    "    a list with the same order as the original.\n",
    "    '''\n",
    "    \n",
    "    processed = []\n",
    "    total = len(raw)\n",
    "    current = 1\n",
    "    for lang in raw:\n",
    "        print('processing {} of {}'.format(current, total))\n",
    "        current += 1\n",
    "\n",
    "        #process w/ BeautifulSoup\n",
    "        print('...parsing')\n",
    "        soup = BeautifulSoup(lang, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "\n",
    "        #continue cleaning — remove whitespace and remove punctuation\n",
    "        print('...cleaning')\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        lower_words = [word.lower() for word in tokens if word.isalpha() and word!='ENDOFARTICLE']\n",
    "        \n",
    "        print('\\n')\n",
    "        processed.append(lower_words)\n",
    "    \n",
    "    print('Done!')\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1 of 2\n",
      "...parsing\n",
      "...cleaning\n",
      "\n",
      "\n",
      "processing 2 of 2\n",
      "...parsing\n",
      "...cleaning\n",
      "\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#takes a while\n",
    "processed_data = process_wikicorpus(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['henry', 'hallam', 'july', 'january', 'was', 'an', 'english', 'historian', 'the', 'only', 'son', 'of', 'john', 'hallam', 'canon', 'of', 'windsor', 'and', 'dean', 'of']\n",
      "\n",
      "************\n",
      "\n",
      "['acontecimientos', 'nacimientos', 'fallecimientos', 'fulgencio', 'de', 'écija', 'santo', 'español', 'erquinoaldo', 'mayordomo', 'franco', 'de', 'palacio', 'de', 'neustria', 'acontecimientos', 'nacimientos', 'egilona', 'última', 'reina']\n"
     ]
    }
   ],
   "source": [
    "print(processed_data[0][:20])    #clean english text\n",
    "print('\\n************\\n')\n",
    "print(processed_data[1][:20])    #clean spanish text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate (cutoff shortest length, rounded down to be divisible by 100)\n",
    "cutoff = len(min(processed_data, key=len))\n",
    "adjusted_cutoff = int(100*np.floor(cutoff/100))\n",
    "\n",
    "processed_data[0] = processed_data[0][:adjusted_cutoff]\n",
    "processed_data[1] = processed_data[1][:adjusted_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#make packets of 100 words\n",
    "num_packets = adjusted_cutoff//100\n",
    "final_data = np.empty(shape=(2, num_packets)).tolist()\n",
    "\n",
    "#join words in packets\n",
    "for i, lang in enumerate(processed_data):\n",
    "    for j in range(num_packets):\n",
    "        final_data[i][j] = ' '.join(processed_data[i][j*100:j*100+100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['a', 'á', 'b', 'c', 'd', 'e', 'é', 'f', 'g', 'h', 'i', 'í', 'j', 'k', 'l', 'm', 'n', \n",
    "           'ñ', 'o', 'ó', 'p', 'q', 'r', 's', 't', 'u', 'ú', 'v', 'w', 'x', 'y', 'z', ' ']\n",
    "\n",
    "def packet_to_matrix(packet, letter_list):\n",
    "    packet_length = len(packet)\n",
    "    letter_length = len(letter_list)\n",
    "    one_hot_packet = np.zeros(packet_length*(letter_length+1))\n",
    "    one_hot_packet = np.reshape(one_hot_packet, (packet_length, letter_length+1))\n",
    "    \n",
    "    for i in range(packet_length):\n",
    "        try:\n",
    "            index = letter_list.index(packet[i])\n",
    "        except ValueError:\n",
    "            index = letter_length-1\n",
    "            \n",
    "        one_hot_packet[i][index] = 1\n",
    "        \n",
    "    return one_hot_packet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrices of letter frequency in each packet which I probably could have done with nltk but oh well\n",
    "letter_freqs = np.zeros([2, num_packets, len(letters)+1])\n",
    "\n",
    "for i, lang in enumerate(final_data):\n",
    "    for j in range(num_packets):\n",
    "        temp = packet_to_matrix(final_data[i][j], letters)\n",
    "        letter_freqs[i][j] = np.sum(temp, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources (so far)\n",
    "https://www.quora.com/How-do-I-read-mutiple-txt-files-from-folder-in-python\n",
    "\n",
    "https://www.w3schools.com/python/python_regex.asp\n",
    "\n",
    "https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "\n",
    "https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
